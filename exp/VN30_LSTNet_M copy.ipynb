{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.optim as optim\n",
    "\n",
    "class Optim(object):\n",
    "\n",
    "    def _makeOptimizer(self):\n",
    "        if self.method == 'sgd':\n",
    "            self.optimizer = optim.SGD(self.params, lr=self.lr)\n",
    "        elif self.method == 'adagrad':\n",
    "            self.optimizer = optim.Adagrad(self.params, lr=self.lr)\n",
    "        elif self.method == 'adadelta':\n",
    "            self.optimizer = optim.Adadelta(self.params, lr=self.lr)\n",
    "        elif self.method == 'adam':\n",
    "            self.optimizer = optim.Adam(self.params, lr=self.lr)\n",
    "        else:\n",
    "            raise RuntimeError(\"Invalid optim method: \" + self.method)\n",
    "\n",
    "    def __init__(self, params, method, lr, max_grad_norm, lr_decay=1, start_decay_at=None):\n",
    "        self.params = list(params)  # careful: params may be a generator\n",
    "        self.last_ppl = None\n",
    "        self.lr = lr\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.method = method\n",
    "        self.lr_decay = lr_decay\n",
    "        self.start_decay_at = start_decay_at\n",
    "        self.start_decay = False\n",
    "\n",
    "        self._makeOptimizer()\n",
    "\n",
    "    def step(self):\n",
    "        # Compute gradients norm.\n",
    "        grad_norm = 0\n",
    "        for param in self.params:\n",
    "            grad_norm += math.pow(param.grad.data.norm(), 2)\n",
    "\n",
    "        grad_norm = math.sqrt(grad_norm)\n",
    "        if grad_norm > 0:\n",
    "            shrinkage = self.max_grad_norm / grad_norm\n",
    "        else:\n",
    "            shrinkage = 1.\n",
    "\n",
    "        for param in self.params:\n",
    "            if shrinkage < 1:\n",
    "                param.grad.data.mul_(shrinkage)\n",
    "\n",
    "        self.optimizer.step()\n",
    "        return grad_norm\n",
    "\n",
    "    # decay learning rate if val perf does not improve or we hit the start_decay_at limit\n",
    "    def updateLearningRate(self, ppl, epoch):\n",
    "        if self.start_decay_at is not None and epoch >= self.start_decay_at:\n",
    "            self.start_decay = True\n",
    "        if self.last_ppl is not None and ppl > self.last_ppl:\n",
    "            self.start_decay = True\n",
    "\n",
    "        if self.start_decay:\n",
    "            self.lr = self.lr * self.lr_decay\n",
    "            print(\"Decaying learning rate to %g\" % self.lr)\n",
    "        #only decay for one epoch\n",
    "        self.start_decay = False\n",
    "\n",
    "        self.last_ppl = ppl\n",
    "\n",
    "        self._makeOptimizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np;\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "\n",
    "def normal_std(x):\n",
    "    return x.std() * np.sqrt((len(x) - 1.)/(len(x)))\n",
    "\n",
    "class Data_utility(object):\n",
    "    # train and valid is the ratio of training set and validation set. test = 1 - train - valid\n",
    "    def __init__(self, file_name, train, valid, cuda, horizon, window, normalize = 2):\n",
    "        self.cuda = cuda;\n",
    "        self.P = window;\n",
    "        self.h = horizon\n",
    "        fin = open(file_name);\n",
    "        df = pd.read_csv('./data/VN30.csv', header=0, index_col=0)\n",
    "        # self.rawdat = np.loadtxt(fin,delimiter=',');\n",
    "        self.rawdat = df.to_numpy()\n",
    "        self.dat = np.zeros(self.rawdat.shape);\n",
    "        self.n, self.m = self.dat.shape;\n",
    "        self.normalize = 2\n",
    "        self.scale = np.ones(self.m);\n",
    "        self._normalized(normalize);\n",
    "        self._split(int(train * self.n), int((train+valid) * self.n), self.n);\n",
    "        \n",
    "        self.scale = torch.from_numpy(self.scale).float();\n",
    "        tmp = self.test[1] * self.scale.expand(self.test[1].size(0), self.m);\n",
    "            \n",
    "        if self.cuda:\n",
    "            self.scale = self.scale.cuda();\n",
    "        self.scale = Variable(self.scale);\n",
    "        \n",
    "        self.rse = normal_std(tmp);\n",
    "        self.rae = torch.mean(torch.abs(tmp - torch.mean(tmp)));\n",
    "    \n",
    "    def _normalized(self, normalize):\n",
    "        #normalized by the maximum value of entire matrix.\n",
    "       \n",
    "        if (normalize == 0):\n",
    "            self.dat = self.rawdat\n",
    "            \n",
    "        if (normalize == 1):\n",
    "            self.dat = self.rawdat / np.max(self.rawdat);\n",
    "            \n",
    "        #normlized by the maximum value of each row(sensor).\n",
    "        if (normalize == 2):\n",
    "            for i in range(self.m):\n",
    "                self.scale[i] = np.max(np.abs(self.rawdat[:,i]));\n",
    "                self.dat[:,i] = self.rawdat[:,i] / np.max(np.abs(self.rawdat[:,i]));\n",
    "            \n",
    "        \n",
    "    def _split(self, train, valid, test):\n",
    "        \n",
    "        train_set = range(self.P+self.h-1, train);\n",
    "        valid_set = range(train, valid);\n",
    "        test_set = range(valid, self.n);\n",
    "        print('===========SPLIT DATA')\n",
    "\n",
    "        self.train = self._batchify(train_set, self.h);\n",
    "        self.valid = self._batchify(valid_set, self.h);\n",
    "        self.test = self._batchify(test_set, self.h);\n",
    "        print(f'TRAIN: {self.train[0].shape}')\n",
    "        print(f'VAL: {self.valid[0].shape}')\n",
    "        print(f'TEST: {self.test[0].shape}')\n",
    "        \n",
    "        \n",
    "    def _batchify(self, idx_set, horizon):\n",
    "        \n",
    "        n = len(idx_set);\n",
    "        X = torch.zeros((n,self.P,self.m));\n",
    "        Y = torch.zeros((n,self.m));\n",
    "        \n",
    "        for i in range(n):\n",
    "            end = idx_set[i] - self.h + 1;\n",
    "            start = end - self.P;\n",
    "            X[i,:,:] = torch.from_numpy(self.dat[start:end, :]);\n",
    "            Y[i,:] = torch.from_numpy(self.dat[idx_set[i], :]);\n",
    "\n",
    "        return [X, Y];\n",
    "\n",
    "    def get_batches(self, inputs, targets, batch_size, shuffle=True):\n",
    "        length = len(inputs)\n",
    "        if shuffle:\n",
    "            index = torch.randperm(length)\n",
    "        else:\n",
    "            index = torch.LongTensor(range(length))\n",
    "        start_idx = 0\n",
    "        while (start_idx < length):\n",
    "            end_idx = min(length, start_idx + batch_size)\n",
    "            excerpt = index[start_idx:end_idx]\n",
    "            X = inputs[excerpt]; Y = targets[excerpt];\n",
    "            if (self.cuda):\n",
    "                X = X.cuda();\n",
    "                Y = Y.cuda();  \n",
    "            yield Variable(X), Variable(Y);\n",
    "            start_idx += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTNet(nn.Module):\n",
    "    def __init__(self, args, data):\n",
    "        super(LSTNet, self).__init__()\n",
    "        self.use_cuda = args.cuda\n",
    "        self.P = args.window;\n",
    "        self.m = data.m\n",
    "        self.hidR = args.hidRNN;\n",
    "        self.hidC = args.hidCNN;\n",
    "        self.hidS = args.hidSkip;\n",
    "        self.Ck = args.CNN_kernel;\n",
    "        self.skip = args.skip;\n",
    "        self.pt = int((self.P - self.Ck)/self.skip)\n",
    "        self.hw = args.highway_window\n",
    "        self.conv1 = nn.Conv2d(1, self.hidC, kernel_size = (self.Ck, self.m));\n",
    "        self.GRU1 = nn.GRU(self.hidC, self.hidR);\n",
    "        self.dropout = nn.Dropout(p = args.dropout);\n",
    "        if (self.skip > 0):\n",
    "            self.GRUskip = nn.GRU(self.hidC, self.hidS);\n",
    "            self.linear1 = nn.Linear(self.hidR + self.skip * self.hidS, self.m);\n",
    "        else:\n",
    "            self.linear1 = nn.Linear(self.hidR, self.m);\n",
    "        if (self.hw > 0):\n",
    "            self.highway = nn.Linear(self.hw, 1);\n",
    "        self.output = None;\n",
    "        if (args.output_fun == 'sigmoid'):\n",
    "            self.output = F.sigmoid;\n",
    "        if (args.output_fun == 'tanh'):\n",
    "            self.output = F.tanh;\n",
    " \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0);\n",
    "        \n",
    "        #CNN\n",
    "        c = x.view(-1, 1, self.P, self.m);\n",
    "        c = F.relu(self.conv1(c));\n",
    "        c = self.dropout(c);\n",
    "        c = torch.squeeze(c, 3);\n",
    "        \n",
    "        # RNN \n",
    "        r = c.permute(2, 0, 1).contiguous();\n",
    "        _, r = self.GRU1(r);\n",
    "        r = self.dropout(torch.squeeze(r,0));\n",
    "\n",
    "        \n",
    "        #skip-rnn\n",
    "        \n",
    "        if (self.skip > 0):\n",
    "            s = c[:,:, int(-self.pt * self.skip):].contiguous();\n",
    "            s = s.view(batch_size, self.hidC, self.pt, self.skip);\n",
    "            s = s.permute(2,0,3,1).contiguous();\n",
    "            s = s.view(self.pt, batch_size * self.skip, self.hidC);\n",
    "            _, s = self.GRUskip(s);\n",
    "            s = s.view(batch_size, self.skip * self.hidS);\n",
    "            s = self.dropout(s);\n",
    "            r = torch.cat((r,s),1);\n",
    "        \n",
    "        res = self.linear1(r);\n",
    "        \n",
    "        #highway\n",
    "        if (self.hw > 0):\n",
    "            z = x[:, -self.hw:, :];\n",
    "            z = z.permute(0,2,1).contiguous().view(-1, self.hw);\n",
    "            z = self.highway(z);\n",
    "            z = z.view(-1,self.m);\n",
    "            res = res + z;\n",
    "            \n",
    "        if (self.output):\n",
    "            res = self.output(res);\n",
    "        return res;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========SPLIT DATA\n",
      "TRAIN: torch.Size([1413, 168, 6])\n",
      "VAL: torch.Size([527, 168, 6])\n",
      "TEST: torch.Size([527, 168, 6])\n",
      "tensor(597.3320)\n",
      "* number of parameters: 19056\n",
      "begin training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noing/anaconda3/envs/venv/lib/python3.8/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| end of epoch   1 | time: 10.23s | train_loss 6823.2288 | valid rse 42.9764 | valid rae 31.2889 | valid corr  0.7329\n",
      "| end of epoch   2 | time:  8.53s | train_loss 1292.4923 | valid rse 26.4425 | valid rae 17.3790 | valid corr  0.7338\n",
      "| end of epoch   3 | time: 11.14s | train_loss 417.6539 | valid rse 26.1664 | valid rae 16.9703 | valid corr  0.7353\n",
      "| end of epoch   4 | time: 11.09s | train_loss 388.4421 | valid rse 25.8746 | valid rae 16.5334 | valid corr  0.7384\n",
      "| end of epoch   5 | time: 10.34s | train_loss 367.4546 | valid rse 25.4497 | valid rae 16.3224 | valid corr  0.7415\n",
      "test rse 45.0224 | test rae 32.1272 | test corr 0.7396\n",
      "| end of epoch   6 | time: 11.33s | train_loss 366.1675 | valid rse 25.0596 | valid rae 16.4686 | valid corr  0.7447\n",
      "| end of epoch   7 | time:  9.47s | train_loss 354.5858 | valid rse 26.3073 | valid rae 16.5896 | valid corr  0.7488\n",
      "| end of epoch   8 | time: 11.28s | train_loss 356.1981 | valid rse 25.6552 | valid rae 16.1594 | valid corr  0.7486\n",
      "| end of epoch   9 | time: 13.14s | train_loss 363.8777 | valid rse 25.9416 | valid rae 16.3787 | valid corr  0.7500\n",
      "| end of epoch  10 | time:  9.75s | train_loss 381.0463 | valid rse 24.1273 | valid rae 15.6782 | valid corr  0.7516\n",
      "test rse 43.1358 | test rae 30.8128 | test corr 0.7444\n",
      "| end of epoch  11 | time: 10.39s | train_loss 336.8905 | valid rse 27.2944 | valid rae 17.5944 | valid corr  0.7524\n",
      "| end of epoch  12 | time:  9.20s | train_loss 397.4922 | valid rse 23.7992 | valid rae 15.5831 | valid corr  0.7541\n",
      "| end of epoch  13 | time:  6.68s | train_loss 322.7610 | valid rse 23.9235 | valid rae 15.2290 | valid corr  0.7557\n",
      "| end of epoch  14 | time: 10.43s | train_loss 317.2206 | valid rse 23.7181 | valid rae 15.0629 | valid corr  0.7582\n",
      "| end of epoch  15 | time: 10.87s | train_loss 310.1877 | valid rse 23.6171 | valid rae 16.1834 | valid corr  0.7605\n",
      "test rse 42.6016 | test rae 31.1651 | test corr 0.7483\n",
      "| end of epoch  16 | time:  7.33s | train_loss 317.1301 | valid rse 23.3728 | valid rae 14.8410 | valid corr  0.7615\n",
      "| end of epoch  17 | time: 10.59s | train_loss 304.6307 | valid rse 22.6153 | valid rae 14.7044 | valid corr  0.7630\n",
      "| end of epoch  18 | time: 10.63s | train_loss 295.3545 | valid rse 23.4081 | valid rae 14.8268 | valid corr  0.7648\n",
      "| end of epoch  19 | time:  6.49s | train_loss 306.8750 | valid rse 23.1153 | valid rae 14.6535 | valid corr  0.7663\n",
      "| end of epoch  20 | time:  8.46s | train_loss 338.5360 | valid rse 24.7649 | valid rae 18.0871 | valid corr  0.7684\n",
      "test rse 44.2372 | test rae 33.1767 | test corr 0.7520\n",
      "| end of epoch  21 | time: 10.01s | train_loss 335.1330 | valid rse 22.4815 | valid rae 15.4816 | valid corr  0.7690\n",
      "| end of epoch  22 | time:  8.21s | train_loss 354.0277 | valid rse 25.7084 | valid rae 16.9701 | valid corr  0.7690\n",
      "| end of epoch  23 | time:  9.65s | train_loss 294.7393 | valid rse 23.4349 | valid rae 14.9586 | valid corr  0.7699\n",
      "| end of epoch  24 | time: 10.48s | train_loss 295.7374 | valid rse 23.3088 | valid rae 14.8928 | valid corr  0.7710\n",
      "| end of epoch  25 | time:  8.65s | train_loss 271.6580 | valid rse 22.4829 | valid rae 15.7952 | valid corr  0.7729\n",
      "test rse 40.8732 | test rae 30.0308 | test corr 0.7545\n",
      "| end of epoch  26 | time:  9.57s | train_loss 274.6870 | valid rse 21.1550 | valid rae 14.0187 | valid corr  0.7738\n",
      "| end of epoch  27 | time: 13.29s | train_loss 264.8136 | valid rse 20.9800 | valid rae 13.6978 | valid corr  0.7747\n",
      "| end of epoch  28 | time:  7.46s | train_loss 261.1262 | valid rse 20.7011 | valid rae 13.5821 | valid corr  0.7766\n",
      "| end of epoch  29 | time:  8.35s | train_loss 256.9342 | valid rse 20.6665 | valid rae 13.3505 | valid corr  0.7778\n",
      "| end of epoch  30 | time:  9.62s | train_loss 248.4918 | valid rse 20.3604 | valid rae 13.3803 | valid corr  0.7787\n",
      "test rse 37.5729 | test rae 26.5488 | test corr 0.7575\n",
      "| end of epoch  31 | time:  8.61s | train_loss 243.0123 | valid rse 20.1241 | valid rae 13.2655 | valid corr  0.7806\n",
      "| end of epoch  32 | time:  5.87s | train_loss 243.7592 | valid rse 20.0497 | valid rae 12.9853 | valid corr  0.7816\n",
      "| end of epoch  33 | time: 10.29s | train_loss 248.2055 | valid rse 21.1368 | valid rae 13.5788 | valid corr  0.7827\n",
      "| end of epoch  34 | time:  9.94s | train_loss 248.8065 | valid rse 19.5807 | valid rae 12.9519 | valid corr  0.7842\n",
      "| end of epoch  35 | time:  7.49s | train_loss 233.3097 | valid rse 19.4311 | valid rae 12.7641 | valid corr  0.7850\n",
      "test rse 36.1947 | test rae 25.4425 | test corr 0.7603\n",
      "| end of epoch  36 | time: 11.15s | train_loss 237.2644 | valid rse 21.3842 | valid rae 15.4970 | valid corr  0.7862\n",
      "| end of epoch  37 | time: 10.20s | train_loss 249.1652 | valid rse 19.4222 | valid rae 13.1627 | valid corr  0.7869\n",
      "| end of epoch  38 | time:  6.74s | train_loss 233.8833 | valid rse 19.9140 | valid rae 13.8893 | valid corr  0.7878\n",
      "| end of epoch  39 | time:  7.21s | train_loss 235.7124 | valid rse 21.0121 | valid rae 15.2445 | valid corr  0.7886\n",
      "| end of epoch  40 | time:  8.83s | train_loss 232.2662 | valid rse 18.9376 | valid rae 12.6674 | valid corr  0.7888\n",
      "test rse 35.5572 | test rae 25.1907 | test corr 0.7622\n",
      "| end of epoch  41 | time:  9.21s | train_loss 216.9933 | valid rse 18.9524 | valid rae 12.2553 | valid corr  0.7895\n",
      "| end of epoch  42 | time:  7.30s | train_loss 215.3922 | valid rse 19.8029 | valid rae 13.9974 | valid corr  0.7906\n",
      "| end of epoch  43 | time: 10.17s | train_loss 222.3892 | valid rse 18.5475 | valid rae 12.4367 | valid corr  0.7914\n",
      "| end of epoch  44 | time: 11.44s | train_loss 221.1056 | valid rse 18.3021 | valid rae 12.0664 | valid corr  0.7922\n",
      "| end of epoch  45 | time: 10.63s | train_loss 204.6023 | valid rse 18.2369 | valid rae 11.8991 | valid corr  0.7929\n",
      "test rse 34.3697 | test rae 23.8592 | test corr 0.7644\n",
      "| end of epoch  46 | time: 11.26s | train_loss 207.2601 | valid rse 17.9626 | valid rae 11.8209 | valid corr  0.7942\n",
      "| end of epoch  47 | time:  9.51s | train_loss 212.7887 | valid rse 19.4306 | valid rae 12.6586 | valid corr  0.7949\n",
      "| end of epoch  48 | time:  8.50s | train_loss 212.2012 | valid rse 18.8394 | valid rae 12.2043 | valid corr  0.7954\n",
      "| end of epoch  49 | time: 10.46s | train_loss 196.6340 | valid rse 17.9296 | valid rae 11.6026 | valid corr  0.7963\n",
      "| end of epoch  50 | time:  9.89s | train_loss 192.5983 | valid rse 17.4862 | valid rae 11.5966 | valid corr  0.7972\n",
      "test rse 33.3723 | test rae 23.2958 | test corr 0.7664\n",
      "| end of epoch  51 | time:  6.71s | train_loss 187.7424 | valid rse 17.9639 | valid rae 11.6422 | valid corr  0.7990\n",
      "| end of epoch  52 | time: 10.19s | train_loss 193.2810 | valid rse 17.9841 | valid rae 11.6745 | valid corr  0.7995\n",
      "| end of epoch  53 | time:  9.98s | train_loss 184.8490 | valid rse 17.3815 | valid rae 11.2612 | valid corr  0.7993\n",
      "| end of epoch  54 | time:  8.80s | train_loss 186.9731 | valid rse 17.5196 | valid rae 11.3592 | valid corr  0.8008\n",
      "| end of epoch  55 | time: 11.19s | train_loss 234.4760 | valid rse 18.6066 | valid rae 12.2516 | valid corr  0.8011\n",
      "test rse 34.4366 | test rae 23.4692 | test corr 0.7686\n",
      "| end of epoch  56 | time: 10.82s | train_loss 190.2476 | valid rse 17.0101 | valid rae 11.0353 | valid corr  0.8015\n",
      "| end of epoch  57 | time:  7.34s | train_loss 175.4306 | valid rse 16.7986 | valid rae 11.3227 | valid corr  0.8020\n",
      "| end of epoch  58 | time: 13.54s | train_loss 186.1217 | valid rse 18.4378 | valid rae 13.4015 | valid corr  0.8030\n",
      "| end of epoch  59 | time:  9.71s | train_loss 204.9160 | valid rse 17.8113 | valid rae 12.7160 | valid corr  0.8033\n",
      "| end of epoch  60 | time:  9.68s | train_loss 175.5325 | valid rse 16.3786 | valid rae 10.7174 | valid corr  0.8035\n",
      "test rse 31.7208 | test rae 21.7517 | test corr 0.7697\n",
      "| end of epoch  61 | time: 11.76s | train_loss 180.7431 | valid rse 16.9208 | valid rae 11.7204 | valid corr  0.8041\n",
      "| end of epoch  62 | time:  8.79s | train_loss 172.9448 | valid rse 17.2064 | valid rae 12.1082 | valid corr  0.8045\n",
      "| end of epoch  63 | time:  8.06s | train_loss 170.9266 | valid rse 16.1274 | valid rae 10.6008 | valid corr  0.8047\n",
      "| end of epoch  64 | time: 11.84s | train_loss 168.4024 | valid rse 16.7045 | valid rae 10.8418 | valid corr  0.8050\n",
      "| end of epoch  65 | time:  9.45s | train_loss 170.6752 | valid rse 16.0155 | valid rae 10.6665 | valid corr  0.8055\n",
      "test rse 31.2802 | test rae 21.6504 | test corr 0.7707\n",
      "| end of epoch  66 | time: 11.39s | train_loss 165.8329 | valid rse 16.4616 | valid rae 10.6797 | valid corr  0.8057\n",
      "| end of epoch  67 | time: 11.44s | train_loss 174.2737 | valid rse 17.6896 | valid rae 11.7002 | valid corr  0.8060\n",
      "| end of epoch  68 | time:  7.98s | train_loss 184.6036 | valid rse 16.7173 | valid rae 10.8872 | valid corr  0.8064\n",
      "| end of epoch  69 | time: 10.10s | train_loss 175.1386 | valid rse 17.7406 | valid rae 11.7701 | valid corr  0.8066\n",
      "| end of epoch  70 | time: 10.22s | train_loss 180.5366 | valid rse 19.8817 | valid rae 13.8593 | valid corr  0.8067\n",
      "test rse 35.7261 | test rae 24.7182 | test corr 0.7715\n",
      "| end of epoch  71 | time:  7.84s | train_loss 260.4690 | valid rse 18.7137 | valid rae 14.0289 | valid corr  0.8075\n",
      "| end of epoch  72 | time:  9.91s | train_loss 175.2470 | valid rse 15.6493 | valid rae 10.4147 | valid corr  0.8074\n",
      "| end of epoch  73 | time: 13.06s | train_loss 156.1692 | valid rse 15.7264 | valid rae 10.2122 | valid corr  0.8079\n",
      "| end of epoch  74 | time:  7.58s | train_loss 155.3434 | valid rse 15.5196 | valid rae 10.0943 | valid corr  0.8082\n",
      "| end of epoch  75 | time:  9.06s | train_loss 152.5227 | valid rse 16.5996 | valid rae 11.8610 | valid corr  0.8092\n",
      "test rse 32.0724 | test rae 23.1279 | test corr 0.7724\n",
      "| end of epoch  76 | time: 10.33s | train_loss 157.7996 | valid rse 16.2306 | valid rae 10.5913 | valid corr  0.8094\n",
      "| end of epoch  77 | time:  9.19s | train_loss 163.1191 | valid rse 17.1529 | valid rae 11.4321 | valid corr  0.8099\n",
      "| end of epoch  78 | time:  9.93s | train_loss 194.0157 | valid rse 15.3610 | valid rae 10.4818 | valid corr  0.8104\n",
      "| end of epoch  79 | time: 10.28s | train_loss 149.6489 | valid rse 15.0098 | valid rae 9.8202 | valid corr  0.8104\n",
      "| end of epoch  80 | time:  8.73s | train_loss 145.2542 | valid rse 15.8323 | valid rae 10.3078 | valid corr  0.8105\n",
      "test rse 30.5087 | test rae 20.3661 | test corr 0.7733\n",
      "| end of epoch  81 | time:  8.87s | train_loss 152.7919 | valid rse 14.9450 | valid rae 9.7313 | valid corr  0.8109\n",
      "| end of epoch  82 | time: 18.50s | train_loss 144.8156 | valid rse 15.0083 | valid rae 9.7396 | valid corr  0.8111\n",
      "| end of epoch  83 | time:  9.05s | train_loss 153.9535 | valid rse 14.8041 | valid rae 9.8399 | valid corr  0.8114\n",
      "| end of epoch  84 | time: 10.86s | train_loss 144.9893 | valid rse 15.7218 | valid rae 11.1212 | valid corr  0.8117\n",
      "| end of epoch  85 | time:  9.63s | train_loss 149.9487 | valid rse 14.6736 | valid rae 9.7055 | valid corr  0.8119\n",
      "test rse 29.4441 | test rae 20.0715 | test corr 0.7737\n",
      "| end of epoch  86 | time:  5.99s | train_loss 141.7620 | valid rse 14.8707 | valid rae 10.1064 | valid corr  0.8123\n",
      "| end of epoch  87 | time:  5.80s | train_loss 141.6965 | valid rse 15.2771 | valid rae 9.9490 | valid corr  0.8126\n",
      "| end of epoch  88 | time:  5.67s | train_loss 152.3194 | valid rse 14.6362 | valid rae 9.8550 | valid corr  0.8129\n",
      "| end of epoch  89 | time:  5.76s | train_loss 138.6105 | valid rse 14.7236 | valid rae 10.0116 | valid corr  0.8131\n",
      "| end of epoch  90 | time:  5.95s | train_loss 153.5573 | valid rse 14.3855 | valid rae 9.4837 | valid corr  0.8133\n",
      "test rse 29.0387 | test rae 19.6883 | test corr 0.7744\n",
      "| end of epoch  91 | time:  5.61s | train_loss 139.2476 | valid rse 15.0233 | valid rae 9.7472 | valid corr  0.8134\n",
      "| end of epoch  92 | time:  5.44s | train_loss 139.8365 | valid rse 14.2850 | valid rae 9.3075 | valid corr  0.8138\n",
      "| end of epoch  93 | time:  5.33s | train_loss 138.5869 | valid rse 14.2434 | valid rae 9.4592 | valid corr  0.8141\n",
      "| end of epoch  94 | time:  5.57s | train_loss 132.6246 | valid rse 14.1544 | valid rae 9.2590 | valid corr  0.8143\n",
      "| end of epoch  95 | time:  5.57s | train_loss 132.2725 | valid rse 14.2393 | valid rae 9.6092 | valid corr  0.8147\n",
      "test rse 28.9160 | test rae 19.8634 | test corr 0.7750\n",
      "| end of epoch  96 | time:  5.93s | train_loss 144.1427 | valid rse 15.4837 | valid rae 11.2028 | valid corr  0.8151\n",
      "| end of epoch  97 | time:  7.67s | train_loss 137.3240 | valid rse 14.0833 | valid rae 9.4591 | valid corr  0.8152\n",
      "| end of epoch  98 | time:  5.09s | train_loss 130.6803 | valid rse 14.9545 | valid rae 9.7741 | valid corr  0.8152\n",
      "| end of epoch  99 | time:  7.54s | train_loss 141.6187 | valid rse 14.0578 | valid rae 9.0996 | valid corr  0.8154\n",
      "| end of epoch 100 | time:  4.43s | train_loss 131.0269 | valid rse 13.8528 | valid rae 9.1707 | valid corr  0.8157\n",
      "test rse 28.3641 | test rae 19.1761 | test corr 0.7754\n",
      "| end of epoch 101 | time:  4.80s | train_loss 131.7776 | valid rse 14.0381 | valid rae 9.5191 | valid corr  0.8158\n",
      "| end of epoch 102 | time:  5.98s | train_loss 131.0433 | valid rse 14.1096 | valid rae 9.1159 | valid corr  0.8159\n",
      "| end of epoch 103 | time: 15.45s | train_loss 128.4572 | valid rse 15.2885 | valid rae 11.1053 | valid corr  0.8162\n",
      "| end of epoch 104 | time:  7.60s | train_loss 161.8167 | valid rse 13.7403 | valid rae 9.1077 | valid corr  0.8162\n",
      "| end of epoch 105 | time:  8.21s | train_loss 127.6104 | valid rse 16.7223 | valid rae 12.6632 | valid corr  0.8164\n",
      "test rse 32.3507 | test rae 23.9907 | test corr 0.7756\n",
      "| end of epoch 106 | time:  5.76s | train_loss 147.7707 | valid rse 15.8901 | valid rae 11.7954 | valid corr  0.8165\n",
      "| end of epoch 107 | time:  5.91s | train_loss 145.7894 | valid rse 15.6540 | valid rae 11.5448 | valid corr  0.8165\n",
      "| end of epoch 108 | time:  6.07s | train_loss 148.0937 | valid rse 15.6020 | valid rae 11.4922 | valid corr  0.8166\n",
      "| end of epoch 109 | time: 10.01s | train_loss 144.0549 | valid rse 15.5931 | valid rae 11.4867 | valid corr  0.8166\n",
      "| end of epoch 110 | time: 12.31s | train_loss 132.0667 | valid rse 14.0633 | valid rae 9.0876 | valid corr  0.8165\n",
      "test rse 28.2772 | test rae 18.5789 | test corr 0.7759\n",
      "| end of epoch 111 | time:  7.96s | train_loss 125.0566 | valid rse 13.8888 | valid rae 9.4929 | valid corr  0.8168\n",
      "| end of epoch 112 | time: 11.16s | train_loss 131.1815 | valid rse 13.8683 | valid rae 9.4898 | valid corr  0.8169\n",
      "| end of epoch 113 | time: 10.73s | train_loss 132.5171 | valid rse 16.3122 | valid rae 11.1946 | valid corr  0.8168\n",
      "| end of epoch 114 | time:  7.82s | train_loss 178.4464 | valid rse 13.6945 | valid rae 9.2746 | valid corr  0.8171\n",
      "| end of epoch 115 | time: 10.70s | train_loss 128.7685 | valid rse 15.0913 | valid rae 10.0103 | valid corr  0.8169\n",
      "test rse 29.2617 | test rae 19.3189 | test corr 0.7761\n",
      "| end of epoch 116 | time: 10.94s | train_loss 157.5605 | valid rse 13.7361 | valid rae 8.8770 | valid corr  0.8171\n",
      "| end of epoch 117 | time:  7.89s | train_loss 123.8008 | valid rse 13.8798 | valid rae 8.9648 | valid corr  0.8173\n",
      "| end of epoch 118 | time: 10.34s | train_loss 122.2978 | valid rse 13.5837 | valid rae 9.2024 | valid corr  0.8176\n",
      "| end of epoch 119 | time: 10.39s | train_loss 120.6509 | valid rse 13.6334 | valid rae 9.3024 | valid corr  0.8178\n",
      "| end of epoch 120 | time:  7.70s | train_loss 127.7466 | valid rse 13.6166 | valid rae 8.7837 | valid corr  0.8178\n",
      "test rse 27.7598 | test rae 18.1686 | test corr 0.7764\n",
      "| end of epoch 121 | time: 10.29s | train_loss 121.1751 | valid rse 13.2691 | valid rae 8.6180 | valid corr  0.8180\n",
      "| end of epoch 122 | time:  9.83s | train_loss 118.5285 | valid rse 14.6712 | valid rae 9.6991 | valid corr  0.8181\n",
      "| end of epoch 123 | time:  8.65s | train_loss 151.8866 | valid rse 13.2904 | valid rae 8.5711 | valid corr  0.8183\n",
      "| end of epoch 124 | time:  8.12s | train_loss 125.3433 | valid rse 13.1289 | valid rae 8.5779 | valid corr  0.8185\n",
      "| end of epoch 125 | time: 10.79s | train_loss 136.4609 | valid rse 13.1412 | valid rae 8.6970 | valid corr  0.8186\n",
      "test rse 27.4514 | test rae 18.3569 | test corr 0.7765\n",
      "| end of epoch 126 | time:  8.65s | train_loss 119.9705 | valid rse 14.0119 | valid rae 9.1249 | valid corr  0.8185\n",
      "| end of epoch 127 | time:  8.68s | train_loss 129.6546 | valid rse 13.0613 | valid rae 8.5158 | valid corr  0.8187\n",
      "| end of epoch 128 | time: 11.71s | train_loss 128.8047 | valid rse 13.2977 | valid rae 9.0164 | valid corr  0.8188\n",
      "| end of epoch 129 | time:  9.24s | train_loss 119.4371 | valid rse 13.4007 | valid rae 8.6407 | valid corr  0.8187\n",
      "| end of epoch 130 | time:  9.60s | train_loss 117.2249 | valid rse 13.2375 | valid rae 8.9732 | valid corr  0.8190\n",
      "test rse 27.6047 | test rae 18.7127 | test corr 0.7766\n",
      "| end of epoch 131 | time: 11.22s | train_loss 122.3867 | valid rse 13.5748 | valid rae 8.7883 | valid corr  0.8189\n",
      "| end of epoch 132 | time:  8.82s | train_loss 118.5028 | valid rse 13.7138 | valid rae 9.6351 | valid corr  0.8193\n",
      "| end of epoch 133 | time: 10.91s | train_loss 135.6260 | valid rse 15.4280 | valid rae 11.5869 | valid corr  0.8194\n",
      "| end of epoch 134 | time:  9.97s | train_loss 126.0320 | valid rse 12.9686 | valid rae 8.5935 | valid corr  0.8193\n",
      "| end of epoch 135 | time:  9.02s | train_loss 115.7789 | valid rse 13.2099 | valid rae 8.5065 | valid corr  0.8193\n",
      "test rse 27.3315 | test rae 17.7842 | test corr 0.7767\n",
      "| end of epoch 136 | time:  8.79s | train_loss 117.0439 | valid rse 13.2944 | valid rae 9.1363 | valid corr  0.8195\n",
      "| end of epoch 137 | time: 11.22s | train_loss 117.6473 | valid rse 13.1932 | valid rae 8.5050 | valid corr  0.8194\n",
      "| end of epoch 138 | time:  9.01s | train_loss 116.9264 | valid rse 12.8883 | valid rae 8.5342 | valid corr  0.8196\n",
      "| end of epoch 139 | time:  8.96s | train_loss 114.3378 | valid rse 12.8248 | valid rae 8.4107 | valid corr  0.8196\n",
      "| end of epoch 140 | time: 13.12s | train_loss 115.7642 | valid rse 12.8280 | valid rae 8.4583 | valid corr  0.8197\n",
      "test rse 27.0115 | test rae 17.8892 | test corr 0.7768\n",
      "| end of epoch 141 | time:  8.79s | train_loss 113.0425 | valid rse 12.7793 | valid rae 8.2894 | valid corr  0.8198\n",
      "| end of epoch 142 | time: 10.00s | train_loss 113.7096 | valid rse 12.8313 | valid rae 8.2685 | valid corr  0.8199\n",
      "| end of epoch 143 | time: 10.29s | train_loss 113.1617 | valid rse 13.2166 | valid rae 8.5229 | valid corr  0.8199\n",
      "| end of epoch 144 | time:  8.04s | train_loss 113.9408 | valid rse 13.2076 | valid rae 8.5222 | valid corr  0.8200\n",
      "| end of epoch 145 | time: 10.46s | train_loss 116.2838 | valid rse 12.9142 | valid rae 8.7439 | valid corr  0.8202\n",
      "test rse 27.1864 | test rae 18.2958 | test corr 0.7768\n",
      "| end of epoch 146 | time:  9.94s | train_loss 113.7258 | valid rse 12.8039 | valid rae 8.5756 | valid corr  0.8203\n",
      "| end of epoch 147 | time:  9.28s | train_loss 113.4836 | valid rse 12.9760 | valid rae 8.3493 | valid corr  0.8202\n",
      "| end of epoch 148 | time:  8.99s | train_loss 114.7540 | valid rse 14.7308 | valid rae 10.9344 | valid corr  0.8204\n",
      "| end of epoch 149 | time: 10.15s | train_loss 132.9350 | valid rse 14.9578 | valid rae 11.1830 | valid corr  0.8204\n",
      "| end of epoch 150 | time:  9.37s | train_loss 132.8369 | valid rse 12.7415 | valid rae 8.5003 | valid corr  0.8204\n",
      "test rse 26.9722 | test rae 17.9780 | test corr 0.7769\n",
      "test rse 26.9722 | test rae 17.9780 | test corr 0.7769\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import argparse\n",
    "import math\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np;\n",
    "import importlib\n",
    "\n",
    "def evaluate(data, X, Y, model, evaluateL2, evaluateL1, batch_size):\n",
    "    model.eval();\n",
    "    # print('=======X Shape=========')\n",
    "    # print(X.shape)\n",
    "    total_loss = 0;\n",
    "    total_loss_l1 = 0;\n",
    "    n_samples = 0;\n",
    "    predict = None;\n",
    "    test = None;\n",
    "    \n",
    "    for X, Y in data.get_batches(X, Y, batch_size, False):\n",
    "        output = model(X);\n",
    "        if predict is None:\n",
    "            predict = output;\n",
    "            test = Y;\n",
    "        else:\n",
    "            predict = torch.cat((predict,output));\n",
    "            test = torch.cat((test, Y));\n",
    "        \n",
    "        scale = data.scale.expand(output.size(0), data.m)\n",
    "        total_loss += evaluateL2(output * scale, Y * scale).item()\n",
    "        total_loss_l1 += evaluateL1(output * scale, Y * scale).item()\n",
    "        n_samples += (output.size(0) * data.m);\n",
    "    # rse = math.sqrt(total_loss / n_samples)/data.rse\n",
    "    rse = math.sqrt(total_loss/n_samples)\n",
    "    # rae = (total_loss_l1/n_samples)/data.rae\n",
    "    rae = total_loss_l1/n_samples\n",
    "    \n",
    "    predict = predict.data.cpu().numpy();\n",
    "    Ytest = test.data.cpu().numpy();\n",
    "    sigma_p = (predict).std(axis = 0);\n",
    "    sigma_g = (Ytest).std(axis = 0);\n",
    "    mean_p = predict.mean(axis = 0)\n",
    "    mean_g = Ytest.mean(axis = 0)\n",
    "    index = (sigma_g!=0);\n",
    "    correlation = ((predict - mean_p) * (Ytest - mean_g)).mean(axis = 0)/(sigma_p * sigma_g + 0.000000000000001);\n",
    "    correlation = (correlation[index]).mean();\n",
    "    return rse, rae, correlation;\n",
    "\n",
    "def train(data, X, Y, model, criterion, optim, batch_size):\n",
    "    model.train();\n",
    "    total_loss = 0;\n",
    "    n_samples = 0;\n",
    "    for X, Y in data.get_batches(X, Y, batch_size, True):\n",
    "        model.zero_grad();\n",
    "        output = model(X);\n",
    "        scale = data.scale.expand(output.size(0), data.m)\n",
    "        loss = criterion(output * scale, Y * scale);\n",
    "        loss.backward();\n",
    "        grad_norm = optim.step();\n",
    "        total_loss += loss.item();\n",
    "        n_samples += (output.size(0) * data.m);\n",
    "    return total_loss / n_samples\n",
    "    \n",
    "parser = argparse.ArgumentParser(description='PyTorch Time series forecasting')\n",
    "parser.add_argument('--data', type=str, default=\"data/exchange_rate/exchange_rate.txt\",\n",
    "                    help='location of the data file')\n",
    "parser.add_argument('--model', type=str, default='LSTNet',\n",
    "                    help='')\n",
    "parser.add_argument('--hidCNN', type=int, default=100,\n",
    "                    help='number of CNN hidden units')\n",
    "parser.add_argument('--hidRNN', type=int, default=100,\n",
    "                    help='number of RNN hidden units')\n",
    "parser.add_argument('--window', type=int, default=24 * 7,\n",
    "                    help='window size')\n",
    "parser.add_argument('--CNN_kernel', type=int, default=6,\n",
    "                    help='the kernel size of the CNN layers')\n",
    "parser.add_argument('--highway_window', type=int, default=24,\n",
    "                    help='The window size of the highway component')\n",
    "parser.add_argument('--clip', type=float, default=10.,\n",
    "                    help='gradient clipping')\n",
    "parser.add_argument('--epochs', type=int, default=100,\n",
    "                    help='upper epoch limit')\n",
    "parser.add_argument('--batch_size', type=int, default=128, metavar='N',\n",
    "                    help='batch size')\n",
    "parser.add_argument('--dropout', type=float, default=0.2,\n",
    "                    help='dropout applied to layers (0 = no dropout)')\n",
    "parser.add_argument('--seed', type=int, default=54321,\n",
    "                    help='random seed')\n",
    "parser.add_argument('--gpu', type=int, default=None)\n",
    "parser.add_argument('--log_interval', type=int, default=2000, metavar='N',\n",
    "                    help='report interval')\n",
    "parser.add_argument('--save', type=str,  default='model/model.pt',\n",
    "                    help='path to save the final model')\n",
    "parser.add_argument('--cuda', type=str, default=True)\n",
    "parser.add_argument('--optim', type=str, default='adam')\n",
    "parser.add_argument('--lr', type=float, default=0.001)\n",
    "parser.add_argument('--horizon', type=int, default=1)\n",
    "parser.add_argument('--skip', type=float, default=24)\n",
    "parser.add_argument('--hidSkip', type=int, default=5)\n",
    "parser.add_argument('--L1Loss', type=bool, default=True)\n",
    "parser.add_argument('--normalize', type=int, default=0)\n",
    "parser.add_argument('--output_fun', type=str, default='sigmoid')\n",
    "# args = parser.parse_args()\n",
    "args = dotdict()\n",
    "args.data = './data/VN30.csv'\n",
    "args.model = 'LSTNet'\n",
    "args.hidCNN = 50\n",
    "args.hidRNN = 50\n",
    "args.window = 168\n",
    "args.CNN_kernel = 6\n",
    "args.highway_window = 24\n",
    "args.clip = 10.\n",
    "args.epochs = 150\n",
    "args.batch_size = 128\n",
    "args.dropout = 0.2\n",
    "args.seed = 54321\n",
    "args.gpu = None\n",
    "args.log_interval = 2000\n",
    "args.save = './save/LSTNet.pt'\n",
    "args.cuda = False\n",
    "args.optim = 'adam'\n",
    "args.lr = 0.001\n",
    "args.horizon = 1\n",
    "args.skip = 24\n",
    "args.hidSkip = 5\n",
    "args.L1Loss = False\n",
    "args.normalize = 0\n",
    "args.output_fun = None\n",
    "args.cuda = args.gpu is not None\n",
    "if args.cuda:\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "# Set the random seed manually for reproducibility.\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    if not args.cuda:\n",
    "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "    else:\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "Data = Data_utility(args.data, 0.6, 0.2, args.cuda, args.horizon, args.window, args.normalize);\n",
    "print(Data.rse);\n",
    "\n",
    "model = LSTNet(args, Data);\n",
    "\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "nParams = sum([p.nelement() for p in model.parameters()])\n",
    "print('* number of parameters: %d' % nParams)\n",
    "\n",
    "if args.L1Loss:\n",
    "    criterion = nn.L1Loss(size_average=False);\n",
    "else:\n",
    "    criterion = nn.MSELoss(size_average=False);\n",
    "evaluateL2 = nn.MSELoss(size_average=False);\n",
    "evaluateL1 = nn.L1Loss(size_average=False)\n",
    "if args.cuda:\n",
    "    criterion = criterion.cuda()\n",
    "    evaluateL1 = evaluateL1.cuda();\n",
    "    evaluateL2 = evaluateL2.cuda();\n",
    "    \n",
    "    \n",
    "best_val = 10000000;\n",
    "optim = Optim(\n",
    "    model.parameters(), args.optim, args.lr, args.clip,\n",
    ")\n",
    "\n",
    "# At any point you can hit Ctrl + C to break out of training early.\n",
    "try:\n",
    "    print('begin training');\n",
    "    for epoch in range(1, args.epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        train_loss = train(Data, Data.train[0], Data.train[1], model, criterion, optim, args.batch_size)\n",
    "        val_loss, val_rae, val_corr = evaluate(Data, Data.valid[0], Data.valid[1], model, evaluateL2, evaluateL1, args.batch_size);\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | train_loss {:5.4f} | valid rse {:5.4f} | valid rae {:5.4f} | valid corr  {:5.4f}'.format(epoch, (time.time() - epoch_start_time), train_loss, val_loss, val_rae, val_corr))\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "\n",
    "        if val_loss < best_val:\n",
    "            with open(args.save, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val = val_loss\n",
    "        if epoch % 5 == 0:\n",
    "            test_acc, test_rae, test_corr  = evaluate(Data, Data.test[0], Data.test[1], model, evaluateL2, evaluateL1, args.batch_size);\n",
    "            print (\"test rse {:5.4f} | test rae {:5.4f} | test corr {:5.4f}\".format(test_acc, test_rae, test_corr))\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')\n",
    "\n",
    "# Load the best saved model.\n",
    "with open(args.save, 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "test_acc, test_rae, test_corr  = evaluate(Data, Data.test[0], Data.test[1], model, evaluateL2, evaluateL1, args.batch_size);\n",
    "print (\"test rse {:5.4f} | test rae {:5.4f} | test corr {:5.4f}\".format(test_acc, test_rae, test_corr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([527, 168, 6])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.zero_grad();\n",
    "y_pred = model(Data.test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_price = y_pred[:, 0].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_price = Data.test[1][:, 0].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.856377"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.mean((pred_price - true_price)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([527, 6])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.test[1].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
